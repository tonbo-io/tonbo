# RFC: Write-Ahead Log (WAL) for Tonbo

- Status: Draft
- Authors: Tonbo team
- Created: 2025-08-24
- Area: Durability, I/O, Recovery

## Summary

Introduce a write-ahead log (WAL) to make Tonbo durable against process crashes and power loss. The WAL persists ingest operations (currently Arrow `RecordBatch` payloads; typed row encoding is deferred but kept in the frame model) with a simple framed format, checksums, and controlled fsync policy. Recovery replays valid frames to rebuild the mutable state and reattach any immutables created prior to the crash. This RFC focuses on the WAL-only MVP (no SSTs/manifest yet), and documents how it compares to the legacy implementation and how it aligns with the MVCC design in RFC 0003.

## Goals

- Crash/power-loss durability with bounded RPO depending on sync policy.
- Idempotent replay; preserve ingest order within a DB instance.
- Minimal coupling to the current in-memory design; future-proof for SST/manifest.

## Non-Goals (MVP)

- On-disk SST/segment formats and a persistent manifest (follow-up RFCs).
- WAL truncation/compaction beyond basic retention.
- Encryption/compression (can be added later).

## Legacy WAL (for context)

Path: `/Users/gwo/Idea/seren`

High-level model:

- Sequential log with framed records supporting fragmentation: `Full/First/Middle/Last` frames per logical record (refer to [the type design of RocksDB](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-File-Format)).
- Records capture ingests and transactional groupings; recovery scans until the first invalid frame and replays valid records.
- Commit timestamps are logical and reassigned on recovery while preserving ingest order.

Selected mechanics:

- CRC-guarded frames; header conveys type and length.
- Transactions buffer local writes; WAL contains multi-frame records; commit makes them visible atomically.
- Recovery rebuilds in-memory structures and reassigns new logical timestamps; uncommitted records at the tail are discarded.

Correctness properties achieved via WAL + MVCC:

- Idempotent replay until a corruption/truncation boundary.
- Atomic visibility of multi-key commits at a single logical timestamp.
- Read-after-recovery preserves write order even if timestamps are reassigned.

## Legacy Risks and Gaps

- Timestamp identity: reassigning commit timestamps on recovery is acceptable for SI but must be documented and may complicate cross-process coordination later.
- Implicit commit grouping: fragment-oriented frames require higher-level grouping to infer commit boundaries; explicit commit markers are clearer for recovery.
- WAL pruning: robust truncation depends on an external snapshot/manifest; without it, logs may grow indefinitely.
- Dynamic schema coupling: schema mismatches lead to recovery failure; schema/versioning needs clear persistence and validation.
- Operational visibility: limited metrics around last durable sequence and recovery truncation points.

## Failure Model

- Crashes anywhere: during append, rotation, flush-to-immutable, or recovery.
- Partial writes: last frame may be incomplete; recovery stops at first invalid frame.
- Filesystem: durability gates enforced via `fsync`/`fdatasync`. No assumptions about cross-file ordering.

## WAL Frame Model

Common frame header (little-endian):
- magic: `u32` (e.g., 0x544F4E57 "TONW")
- version: `u16` (start with 1)
- frame_type: `u16`
- seq: `u64` (monotonic frame sequence)
- len: `u32` (payload length in bytes)
- crc32c: `u32` (header+payload or payload-only; pick one and document)

Frame types (payload formats):
- `TxnBegin { provisional_id: u64 }`: starts a transactional group. Optional in autocommit mode.
- `TxnAppend { provisional_id: u64, mode: Dyn, payload }`: appends batches to the open txn.
  - `mode=Dyn`: Arrow IPC message for a `RecordBatch`. The first segment per DB stores the schema; subsequent batches can rely on that schema.
  - `mode=Typed` remains reserved so we can reintroduce compile-time typed dispatch without mutating the frame model; the encoding will mirror the legacy `(Key, Payload)` layout when reinstated.
- `TxnCommit { provisional_id: u64, commit_ts: u64 }`: commits the txn and makes its mutations durable and visible atomically.
- `TxnAbort { provisional_id: u64 }` (optional): explicit abort; absence of commit on crash implies abort.
- `SealMarker` (optional): advisory boundary written when sealing occurs. Recovery can ignore this in MVP.

Autocommit fast path:

- An implementation MAY encode a single-operation autocommit as `TxnAppend` immediately followed by `TxnCommit` with a short-lived `provisional_id`, or define a convenience `AppendAutocommit { commit_ts, ... }` that is equivalent on recovery.

## File Layout and Rotation

- Directory: `wal/` under the DB root.
- File naming: `wal-<start_seq>.tonwal` (start sequence encoded in the name for ordering).
- File header: magic (`"TONWAL1"`), version, and optional metadata blocks:
  - dynamic-mode schema digest and optionally an embedded schema block (first file).
  - optional `LastCommitTs` control record; recovery can also derive it as `max(commit_ts)`.
- Rotation: on reaching `segment_max_bytes` or time-based; start a new file with header.
- Retention: MVP keeps recent N segments or caps total WAL bytes. Truncation becomes robust when a manifest/SST snapshot exists.

## Writer Pipeline and Sync Policy

- Single appender with bounded MPSC queue.
- Coalesce small frames; flush on size or timer (e.g., 5â€“20 ms).
- Sync policy:
  - `SyncAlways`: `fsync` every flush (lowest RPO, slowest).
  - `SyncInterval{ bytes, millis }`: sync on thresholds.
  - `SyncDisabled`: dev/test only.
- Error handling: on write/fsync error, surface to caller and potentially switch DB to read-only until resolved.
- Acknowledgement: in autocommit, ack after `TxnCommit` (or `AppendAutocommit`) is durably flushed per policy; in explicit transactions, ack `commit()` only after `TxnCommit` is durable.

## Recovery Algorithm

1. Discover WAL files by name; sort by `start_seq`. Validate headers.
2. Sequentially scan frames:
   - Verify `len` and `crc32c`; stop at the first invalid/incomplete frame.
   - Maintain an in-memory map `provisional_id -> buffered appends`.
   - `TxnBegin`: create an empty buffer for `provisional_id`.
   - `TxnAppend`: decode payload and append to the buffer for `provisional_id`.
   - `TxnCommit`: apply buffered appends to the mutable store using the provided `commit_ts`; clear the buffer.
   - `TxnAbort`: drop any buffered appends.
   - `SealMarker`: optional bookkeeping; can be ignored initially.
3. If recovery ends with open provisional txns (no commit), discard them.
4. Track last applied sequence and compute `last_commit_ts = max(commit_ts)` across commits; initialize the in-memory commit counter to `last_commit_ts + 1`.
5. On schema mismatch (dynamic), abort with a clear error.

## Integration Points

- DB init (dynamic): first WAL segment persists schema and extractor metadata (e.g., key column index + type). Validate on open.
- DB init (typed): persist derived Arrow schema or a type hash for diagnostics.
- Insert path: enqueue WAL frame before acknowledging ingest; acknowledgment timing depends on sync policy.
- Sealing: optionally log `SealMarker` when sealing occurs. Flushing to immutables remains in-memory for MVP; WAL is the durability source.
- MVCC alignment: the `commit_ts` written in `TxnCommit` is the MVCC `begin_ts` for versions; see RFC 0003. Autocommit uses one `commit_ts` per ingest; explicit transactions assign one `commit_ts` to all buffered mutations.

## Legacy and External Reference

| Aspect | Legacy Tonbo WAL | RFC Direction | External Inspiration |
| --- | --- | --- | --- |
| Framing | `Full/First/Middle/Last` fragments encode multi-record batches on a sequential log | `TxnBegin/TxnAppend/TxnCommit` frames keep fragmenting for large payloads but make commit intent explicit | RocksDB fragments WriteBatches per 32 KiB block and reassembles them for atomic replay |
| Commit timestamps | Logical timestamps reassigned during recovery | `commit_ts` persisted per transaction; recovery seeds `commit_clock` from the max observed value | RocksDB TransactionDB records commit metadata so recovery never invents timestamps |
| Object storage flow | Legacy flushed via local fusio logger then copied into remote storage | `WalStorage` relies on fusio's uniform append/commit semantics regardless of backend | RocksDB recommends staging WAL locally before uploading, but fusio now abstracts that detail |
| Multi-family support | Single family; typed modes planned | Frame header will carry logical family/mode identifiers so one WAL serves multiple memtables atomically | RocksDB tags each WriteBatch entry with a column-family ID inside a shared WAL |
| Retention control | Manual or implicit; manifest unaware | Manifest / `wal/state.json` will record highest durable sequence per family before reclaiming segments | RocksDB MANIFEST maintains `min_flush_seq` to gate WAL deletion |
| Telemetry & knobs | Minimal observability | `WalConfig` exposes queue size, sync policy, rotation; metrics track queue depth, bytes, fsync latency | RocksDB surfaces WAL size, write stall, and sync counters; mirroring this keeps ops expectations aligned |

## MVCC Alignment and Changes

- Timestamp type: use `u64` for `commit_ts` to match RFC 0003.
- Visibility: recovery applies only committed txns; incomplete txns are ignored, matching snapshot isolation.
- Autocommit vs transactions: MVP may start with autocommit-only encoding; the frame model is forward-compatible with explicit transactions without changing recovery rules.

## Implementation Outline

We will implement the WAL on top of the `fusio` I/O layer so Tonbo can target any supported storage backend without bespoke branching while staying fully asynchronous.

### Async writer and executor

- WAL APIs (`DB::enable_wal`, `_wal.append_frame`, recovery) become `async`. The WAL service owns an
`Executor + Timer` handle (`fusio::executor`) so it can spawn background tasks without tying Tonbo to a specific runtime. A bounded channel feeds frames from ingest into the writer task; back pressure surfaces when the writer falls behind.
- The writer task tracks ingest order and materializes frames into `Vec<u8>` payloads (dynamic today, typed once reinstated) before handing them to the storage backend.

### Backend abstraction

- Wrap `Arc<dyn fusio::DynFs>` inside a lightweight `WalStorage` facade. Rely directly on fusio's `DynFile`/`DynWrite` traits for append, flush, and close semantics instead of layering a custom backend trait.
- Provide helpers for segment naming, rotation, and retention that operate purely through the fusio API surface; backend-specific behaviors (append vs. multipart upload, durability guarantees, etc.) remain encapsulated by fusio itself.
- Recovery logic reuses the same fusio handles to enumerate segments, stream frames with `Read::read_exact_at`, stop at the first invalid CRC, and rebuild mutable state.

### Sync policy mapping

- `WalSyncPolicy` is enforced in the writer by counting bytes and/or time since the last durability call:
  - `Always` â‡’ call `sync_all` after each committed frame (or after the enclosing transaction).
  - `IntervalBytes(n)` â‡’ accumulate byte count and call `sync_data` once the threshold is exceeded.
  - `IntervalTime(dt)` â‡’ use the executorâ€™s timer to trigger `sync_data` at most once per interval.
- `Disabled` â‡’ rely on backend buffering; `seal` still calls `sync_all`/`commit`.
- If a backend reports limited durability capabilities through fusio, the writer translates the configured policy to the closest supported variant and emits telemetry so operators understand the trade-off.

### Backpressure and metrics

- The bounded channel between ingest and writer naturally applies backpressure. Metrics track queue depth, bytes written, sync counts, and the gap between enqueued and durable sequence numbers.
- The writer task emits structured logs when sync calls exceed a latency budget or when durability is downgraded by the backend.

### Testing strategy

- Unit tests for frame encode/decode and CRC.
- Integration tests across multiple fusio backends:
  - dynamic `RecordBatch` append/replay
  - truncation resilience (simulate torn tail)
  - sync policy adherence (inject slow sync + check counters)
- Future: typed row append/replay once compile-time dispatch returns.

## Engineering Task Breakdown

- **Frame & codec**: finalize the `FrameHeader`, `FrameType`, and payload encoding, including schema bootstrap logic and CRC coverage; deliver unit tests plus fuzz targets for decode.
- **Backend integration**: implement the `WalStorage` facade on top of `fusio::Fs`, covering append, `sync_data`, rotation, and retention trimming while exposing structured errors for upstream.
- **Async writer**: implement `wal::writer` queue, sync policy evaluator, and `WalHandle` submission path; integrate metrics hooks and ensure bounded backpressure semantics.
- **DB integration**: wire `DB::enable_wal`/`disable_wal`, replay on open, and enqueue during ingest; add configuration plumbing so `DB` construction fails fast when the WAL is required but absent.
- **Recovery**: author `wal::replay::scan` and the fold that rebuilds mutable + immutable state; add validation tests for torn tails and mismatched schema digests.
- **Observability & QA**: surface metrics/logging, craft failure-injection tests (slow fsync, simulated backend errors), and document operational runbooks in `docs/`.

## Open Questions

- IPC vs. custom encoding for typed rows? (IPC batches unify both paths but may cost more on small appends.)
- Single WAL per DB vs. per-mode? (Prefer single; schema metadata disambiguates.)
- When to allow WAL truncation without SST/manifest? (Probably only manual for MVP.)

## Rollout Plan

1. Implement frame encode/decode + writer with `fsync` policy and rotation.
2. Integrate enqueue-on-ingest and ack based on policy.
3. Add recovery path on DB open.
4. Add tests + failure injection; add metrics and logs.
5. Document operational guidance and defaults.

## Future Work

- Checkpointing: periodic immutable snapshot + manifest, enabling WAL pruning.
- On-disk immutable segments (Arrow IPC/Parquet) and versioned manifest.
- Optional per-frame compression (zstd) and at-rest encryption.

## Appendices

### Appendix A: Testing and Failure Injection

- Unit: headers, frame encode/decode, CRC verification, truncation behavior.
- Integration:
  - Typed: insert rows, hard-stop before seal; reopen and recover; keys match.
  - Dynamic: insert batch, truncate last frame; replay all valid frames only.
  - Policies: ensure policy-driven seals do not break recovery; immutables may be missing post-crash but mutable is correct.
- Failure injection: simulate partial headers/payloads; disable sync to measure expected RPO.

### Appendix B: Operational Controls and Metrics

- Counters: frames/bytes written, fsync count and latency, queue depth, last durable seq.
- Config knobs: segment size, flush interval, sync policy, retention bytes, queue size.
- Logs: CRC failures, schema mismatch, rotation start/end, recovery truncation points.

### Appendix C: Configuration API Sketch (Rust)

```rust
pub enum WalSyncPolicy {
  Always,           // fsync every flush
  IntervalBytes(usize),    // fsync after N bytes
  IntervalTime(std::time::Duration),
  Disabled,          // dev/test only
}

pub struct WalConfig {
  pub dir: std::path::PathBuf,  // e.g., db_root.join("wal")
  pub segment_max_bytes: usize, // e.g., 64 MiB
  pub flush_interval: std::time::Duration, // e.g., 10 ms
  pub sync: WalSyncPolicy,
  pub retention_bytes: Option<usize>,   // None = keep all
  pub queue_size: usize,          // e.g., 64k frames
}

impl Default for WalConfig {
  fn default() -> Self {
    Self {
      dir: std::path::PathBuf::from("wal"),
      segment_max_bytes: 64 * 1024 * 1024,
      flush_interval: std::time::Duration::from_millis(10),
      sync: WalSyncPolicy::IntervalTime(std::time::Duration::from_millis(50)),
      retention_bytes: None,
      queue_size: 65_536,
    }
  }
}

// Sketchy builder-style usage (no code yet):
// let exec = std::sync::Arc::new(fusio::executor::BlockingExecutor::default());
// let mut db = DB::new_dyn_with_key_name(schema, "id", exec)?;
// db.enable_wal(WalConfig { dir: root.join("wal"), ..Default::default() });
// db.ingest(batch).await?; // ack depends on sync policy
```

### Appendix D: API Surface, Modules, and Runtime Semantics

#### Module layout
- `src/wal/mod.rs`: public entry points (`WalConfig`, `WalHandle`, error types) and re-exports.
- `src/wal/frame.rs`: frame header/payload structs, versioning, CRC helpers.
- `src/wal/mod.rs`: public entry points (`WalConfig`, `WalHandle`, `WalTicket`, `WalError`) and the future writer loop (currently `Unimplemented`).
- `src/wal/backend.rs`: `WalBackend` trait plus the POSIX filesystem implementation.
- `src/wal/replay.rs`: recovery scanner that yields decoded `WalEvent`s back to `DB`.
- `src/wal/metrics.rs`: telemetry structs/hooks; compiled out when metrics feature disabled.

#### Integration status and ownership
- Public API (`WalConfig`, `WalSyncPolicy`, `WalHandle::submit/rotate`, `WalTicket::durable`, `WalPayload`, `WalAck`) is now frozen; request CTO review before altering signatures or semantics.
- Current scaffolding wires the handle into `DB::enable_wal`/`disable_wal` and exposes storage/replay entry points. The writer queue, storage operations, sync policy enforcement, and recovery scanning are intentionally left as `WalError::Unimplemented` for the WAL feature team to implement.
- `WalStorage` must eventually cover directory bootstrap, segment lifecycle, and retention; `Replayer::scan` will drive recovery. Add concrete behavior plus tests in follow-on patches while preserving the documented API surface.
- `WalMetrics` exists as a placeholder structâ€”hook counters into the writer/storage once they land and extend the struct as telemetry gaps appear.

#### Public API (Rust)

```rust
pub use wal::{WalConfig, WalSyncPolicy, WalHandle, WalTicket, WalError};

impl<M: Mode, E: fusio::executor::Executor + fusio::executor::Timer> DB<M, E> {
  pub async fn enable_wal(&mut self, cfg: WalConfig) -> Result<WalHandle<E>, WalError>;

  pub async fn disable_wal(&mut self) -> Result<(), WalError>;

  pub fn wal(&self) -> Option<&WalHandle<E>>;
}

pub enum WalPayload {
  DynBatch { batch: RecordBatch, commit_ts: u64 },
  // Reserved: TypedRow once compile-time dispatch returns.
}

impl<E> WalHandle<E> {
  pub async fn submit(&self, payload: WalPayload) -> Result<WalTicket<E>, WalError>;
  pub async fn rotate(&self) -> Result<(), WalError>; // manual rotation hook
}

pub struct WalTicket<E> {
  pub seq: u64, // frame sequence assigned on enqueue
}

impl<E> WalTicket<E> {
  pub async fn durable(&self) -> Result<WalAck, WalError>; // resolves when fsync policy satisfied
}
```

Key semantics:
- `enable_wal` installs the writer task and replays existing segments before returning the handle.
- `WalHandle::submit` awaits enqueueing (and any backpressure) before returning the ticket; the caller can then `await` `WalTicket::durable()` to honor the configured durability policy.
- `WalTicket::durable().await` resolves with an `Ack` containing elapsed time and bytes flushed; ingest paths can propagate this to the user if needed.

#### Runtime semantics
- Only one writer task exists per `DB`. Ingest operations must pass through the WAL before mutating in-memory state; a write is considered visible once the ticket resolves.
- Backpressure: when the bounded queue is full, `submit` blocks (async) until capacity exists, so user-facing ingest futures naturally wait.
- Disabling the WAL awaits the writer to drain, seals the active segment, and prevents new submit calls. Re-enabling replays again.
- Error propagation: `WalError::Storage` surfaces permanent I/O failures; the DB transitions into `ErrState` and rejects further ingest until reset.

#### Recovery semantics
- `replay::scan(cfg.dir)` produces `(WalEvent, seq)` tuples ordered by `seq`, stopping at the first invalid frame. `WalEvent::DynBatch` exposes the batch and `commit_ts`.
- The `DB` applies each event by reusing the existing dynamic ingest path; this guarantees replay observes the same validation logic as live ingest.
- Recovery records the highest `commit_ts` and seed sequence numbers so subsequent appends remain monotonic.
- Optional seal markers are advisory; missing markers do not block recovery.

#### Observability hooks
- `WalMetrics` exposes queue depth, pending vs durable sequence, sync latency p95/p99, and number of degraded-policy downgrades reported by backends.
- Structured logs (`wal::event::LogRecord`) capture rotations, sync thresholds, and recovery truncation decisions and are emitted through the shared tracing facade.
- Feature flag `metrics` controls whether metrics glue compiles; the API remains present but returns no-op handles without the feature.
