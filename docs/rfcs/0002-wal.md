# RFC: Write-Ahead Log (WAL) for Tonbo

- Status: Draft
- Authors: Tonbo team
- Created: 2025-08-24
- Area: Durability, I/O, Recovery

## Summary

Introduce a write-ahead log (WAL) to make Tonbo durable against process crashes and power loss. The WAL persists ingest operations (currently Arrow `RecordBatch` payloads; typed row encoding is deferred but kept in the frame model) with a simple framed format, checksums, and controlled fsync policy. Recovery replays valid frames to rebuild the mutable state and reattach any immutables created prior to the crash. This RFC focuses on the WAL-only MVP (single-writer queue, no SSTs/manifest yet) and documents how it compares to the legacy implementation and how it aligns with the MVCC design in RFC 0003. Conditional publish (CAS) remains a hard requirement so multi-writer deployments can be enabled in follow-ups without redesigning the frame or storage model.

## Goals

- Crash/power-loss durability with bounded RPO depending on sync policy.
- Idempotent replay; preserve ingest order within a DB instance.
- Minimal coupling to the current in-memory design; future-proof for SST/manifest.

## Non-Goals

- On-disk SST/segment formats and a persistent manifest (follow-up RFCs)
- WAL truncation/compaction beyond basic retention
- Encryption/compression (can be added later)

## Legacy WAL (for context)

Path: `/Users/gwo/Idea/seren`

High-level model:

- Sequential log with framed records supporting fragmentation: `Full/First/Middle/Last` frames per logical record (refer to [the type design of RocksDB](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-File-Format)).
- Records capture ingests and transactional groupings; recovery scans until the first invalid frame and replays valid records.
- Commit timestamps are logical and reassigned on recovery while preserving ingest order.

Selected mechanics:

- CRC-guarded frames; header conveys type and length.
- Transactions buffer local writes; WAL contains multi-frame records; commit makes them visible atomically.
- Recovery rebuilds in-memory structures and reassigns new logical timestamps; uncommitted records at the tail are discarded.

Correctness properties achieved via WAL + MVCC:

- Idempotent replay until a corruption/truncation boundary.
- Atomic visibility of multi-key commits at a single logical timestamp.
- Read-after-recovery preserves write order even if timestamps are reassigned.

## Legacy Risks and Gaps

- Timestamp identity: reassigning commit timestamps on recovery is acceptable for SI but must be documented and may complicate cross-process coordination later.
- Implicit commit grouping: fragment-oriented frames require higher-level grouping to infer commit boundaries; explicit commit markers are clearer for recovery.
- WAL pruning: robust truncation depends on an external snapshot/manifest; without it, logs may grow indefinitely.
- Dynamic schema coupling: schema mismatches lead to recovery failure; schema/versioning needs clear persistence and validation.
- Operational visibility: limited metrics around last durable sequence and recovery truncation points.

## Failure Model

- Crashes anywhere: during append, rotation, flush-to-immutable, or recovery.
- Partial writes: last frame may be incomplete; recovery stops at first invalid frame.
- Filesystem: durability gates enforced via `fsync`/`fdatasync`. No assumptions about cross-file ordering.

## WAL Frame Model

Common frame header (little-endian):
- magic: `u32` (e.g., 0x544F4E57 "TONW")
- version: `u16` (start with 1)
- frame_type: `u16`
- seq: `u64` (monotonic frame sequence)
- len: `u32` (payload length in bytes)
- crc32c: `u32` (header+payload or payload-only; pick one and document)

Frame types (payload formats):
- `TxnBegin { provisional_id: u64 }`: starts a transactional group. Optional in autocommit mode.
- `TxnAppend { provisional_id: u64, mode: Dyn, row_mode, payload }`: appends batches to the open txn.
  - Payload prefix: `provisional_id: u64`, `mode: u8`, `row_mode: u8`, `reserved: [u8; 6]`. The `row_mode` byte consumes the first slot of the reserved block that previously followed `mode`; remaining bytes stay zero until reused.
  - `mode=Dyn` retains the Arrow IPC message for dynamic `RecordBatch` payloads; the schema travels with every payload (IPC stream header + single batch message).
  - `mode=Typed` remains reserved so we can reintroduce compile-time typed dispatch without mutating the frame model; the encoding will mirror the legacy `(Key, Payload)` layout when reinstated.
  - Row modes (applies only to `mode=Dyn` for now):
    - `RowPayload (0)`: payload is the Arrow IPC batch containing all row columns plus an appended `_commit_ts` (`UInt64`) sidecar.
    - `KeyDelete (1)`: key-only deletes. Payload is an Arrow IPC batch encoded with the **delete schema**: the same primary-key columns defined by the extractor followed by a trailing `_commit_ts` (`UInt64`) column. `_tombstone` is never present. Writers must still encode per-row timestamps (constant columns are fine). Replayers reconstruct delete intents directly from the keys + commit timestamps without needing placeholder value columns.
  - Legacy segments implicitly carry `row_mode = RowPayload` because the reserved byte was zeroed; new decoders treat “missing byte” as `RowPayload` for forward/backward compatibility.
- `TxnCommit { provisional_id: u64, commit_ts: u64 }`: commits the txn and makes its mutations durable and visible atomically.
- `TxnAbort { provisional_id: u64 }` (optional): explicit abort; absence of commit on crash implies abort.
- `SealMarker` (optional): advisory boundary written when sealing occurs. Recovery can ignore this in MVP.

Autocommit fast path:

- An implementation MAY encode a single-operation autocommit as `TxnAppend` immediately followed by `TxnCommit` with a short-lived `provisional_id`, or define a convenience `AppendAutocommit { commit_ts, ... }` that is equivalent on recovery.

## File Layout and Rotation

- Directory: `wal/` under the DB root.
- File naming: `wal-<start_seq>.tonwal` (start sequence encoded in the name for ordering).
- File header: magic (`"TONWAL1"`), version, and optional metadata blocks:
  - dynamic-mode schema digest and optionally an embedded schema block (first file).
  - optional `LastCommitTs` control record; recovery can also derive it as `max(commit_ts)`.
- Rotation: on reaching `segment_max_bytes` or time-based; start a new file with header.
- Retention: MVP keeps recent N segments or caps total WAL bytes. Truncation becomes robust when a manifest/SST snapshot exists.

## Writer Pipeline and Sync Policy

- Single appender with bounded MPSC queue.
- Coalesce small frames; flush on size or timer (e.g., 5–20 ms).
- Sync policy:
  - `SyncAlways`: `fsync` every flush (lowest RPO, slowest).
  - `SyncInterval{ bytes, millis }`: sync on thresholds.
  - `SyncDisabled`: dev/test only.
- Error handling: on write/fsync error, surface to caller and potentially switch DB to read-only until resolved.
- Acknowledgement: in autocommit, ack after `TxnCommit` (or `AppendAutocommit`) is durably flushed per policy; in explicit transactions, ack `commit()` only after `TxnCommit` is durable.

## Recovery Algorithm

1. Discover WAL files by name; sort by `start_seq`. Validate headers.
2. Sequentially scan frames:
   - Verify `len` and `crc32c`; stop at the first invalid/incomplete frame.
   - Maintain an in-memory map `provisional_id -> buffered appends`.
   - `TxnBegin`: create an empty buffer for `provisional_id`.
   - `TxnAppend`: decode payload and append to the buffer for `provisional_id`.
   - `TxnCommit`: apply buffered appends to the mutable store using the provided `commit_ts`; clear the buffer.
   - `TxnAbort`: drop any buffered appends.
   - `SealMarker`: optional bookkeeping; can be ignored initially.
3. If recovery ends with open provisional txns (no commit), discard them.
4. Apply committed batches into the mutable memtable so WAL-backed rows are immediately part of the read path alongside txn/mutable/immutable/SST sources.
5. Track last applied sequence and compute `last_commit_ts = max(commit_ts)` across commits; initialize the in-memory commit counter to `last_commit_ts + 1`.
6. On schema mismatch (dynamic), abort with a clear error.

### Transaction persistence behavior

- Live commits: the transaction façade buffers mutations (via `StagedMutations`), emits `TxnBegin/TxnAppend`, waits for the `TxnCommit` frame to become durable, then applies rows to the mutable memtable and publishes manifest edits. This guarantees every committed transaction has a WAL-backed proof before it touches in-memory state.
- Recovery: replay reconstructs the same transaction buffers from the WAL, and only once a durable `TxnCommit` is observed does the recovery flow hand the payload to the transaction logic to apply into the mutable memtable + manifest. Uncommitted provisionals never reach the memtable.
- Result: the effective pipeline becomes `WAL -> transaction -> mutable memtable` (rather than the legacy direct WAL→memtable apply), so durability and snapshot semantics are identical whether the system is running normally or recovering.

## Integration Points

- DB init (dynamic): first WAL segment persists schema and extractor metadata (e.g., key column index + type). Validate on open.
- DB init (typed): persist derived Arrow schema or a type hash for diagnostics.
- Insert path: enqueue WAL frame before acknowledging ingest; acknowledgment timing depends on sync policy.
- Sealing: optionally log `SealMarker` when sealing occurs. Flushing to immutables remains in-memory for MVP; WAL is the durability source.
- MVCC alignment: the `commit_ts` written in `TxnCommit` is the MVCC `begin_ts` for versions; see RFC 0003. Autocommit uses one `commit_ts` per ingest; explicit transactions assign one `commit_ts` to all buffered mutations.

## Legacy and External Reference

| Aspect | Legacy Tonbo WAL | RFC Direction | External Inspiration |
| --- | --- | --- | --- |
| Framing | `Full/First/Middle/Last` fragments encode multi-record batches on a sequential log | `TxnBegin/TxnAppend/TxnCommit` frames keep fragmenting for large payloads but make commit intent explicit | RocksDB fragments WriteBatches per 32 KiB block and reassembles them for atomic replay |
| Commit timestamps | Logical timestamps reassigned during recovery | `commit_ts` persisted per transaction; recovery seeds `commit_clock` from the max observed value | RocksDB TransactionDB records commit metadata so recovery never invents timestamps |
| Object storage flow | Legacy flushed via local fusio logger then copied into remote storage | `WalStorage` relies on fusio's uniform append/commit semantics regardless of backend | RocksDB recommends staging WAL locally before uploading, but fusio now abstracts that detail |
| Multi-family support | Single family; typed modes planned | Frame header will carry logical family/mode identifiers so one WAL serves multiple memtables atomically | RocksDB tags each WriteBatch entry with a column-family ID inside a shared WAL |
| Retention control | Manual or implicit; manifest unaware | Manifest / `wal/state.json` will record highest durable sequence per family before reclaiming segments | RocksDB MANIFEST maintains `min_flush_seq` to gate WAL deletion |
| Telemetry & knobs | Minimal observability | `WalConfig` exposes queue size, sync policy, rotation; metrics track queue depth, bytes, fsync latency | RocksDB surfaces WAL size, write stall, and sync counters; mirroring this keeps ops expectations aligned |

## MVCC Alignment and Changes

- Timestamp type: use `u64` for `commit_ts` to match RFC 0003.
- Visibility: recovery applies only committed txns; incomplete txns are ignored, matching snapshot isolation.
- Autocommit vs transactions: MVP may start with autocommit-only encoding; the frame model is forward-compatible with explicit transactions without changing recovery rules.

## Implementation Outline

We will implement the WAL on top of the `fusio` I/O layer so Tonbo can target any supported storage backend without bespoke branching while staying fully asynchronous.

### Async writer and executor

- WAL APIs (`DB::enable_wal`, `_wal.append_frame`, recovery) become `async`. The WAL service owns an
`Executor + Timer` handle (`fusio::executor`) so it can spawn background tasks without tying Tonbo to a specific runtime. A bounded channel feeds frames from ingest into the writer task; back pressure surfaces when the writer falls behind.
- The writer task tracks ingest order and materializes frames into `Vec<u8>` payloads (dynamic today, typed once reinstated) before handing them to the storage backend.

### Backend abstraction

- Wrap `Arc<dyn fusio::DynFs>` inside a lightweight `WalStorage` facade. Rely directly on fusio's `DynFile`/`DynWrite` traits for append, flush, and close semantics instead of layering a custom backend trait.
- Provide helpers for segment naming, rotation, and retention that operate purely through the fusio API surface; backend-specific behaviors (append vs. multipart upload, durability guarantees, etc.) remain encapsulated by fusio itself.
- Recovery logic reuses the same fusio handles to enumerate segments, stream frames with `Read::read_exact_at`, stop at the first invalid CRC, and rebuild mutable state.

### Sync policy mapping

- `WalSyncPolicy` is enforced in the writer by counting bytes and/or time since the last durability call:
  - `Always` ⇒ call `sync_all` after each committed frame (or after the enclosing transaction).
  - `IntervalBytes(n)` ⇒ accumulate byte count and call `sync_data` once the threshold is exceeded.
  - `IntervalTime(dt)` ⇒ use the executor’s timer to trigger `sync_data` at most once per interval.
- `Disabled` ⇒ rely on backend buffering; `seal` still calls `sync_all`/`commit`.
- If a backend reports limited durability capabilities through fusio, the writer translates the configured policy to the closest supported variant and emits telemetry so operators understand the trade-off.

### Backpressure and metrics

- The bounded channel between ingest and writer naturally applies backpressure. Metrics track queue depth, bytes written, sync counts, and the gap between enqueued and durable sequence numbers.
- The writer task emits structured logs when sync calls exceed a latency budget or when durability is downgraded by the backend.

### Testing strategy

- Unit tests for frame encode/decode and CRC.
- Integration tests across multiple fusio backends:
  - dynamic `RecordBatch` append/replay
  - truncation resilience (simulate torn tail)
  - sync policy adherence (inject slow sync + check counters)
- Future: typed row append/replay once compile-time dispatch returns.

## Engineering Task Breakdown

- **Frame & codec**: finalize the `FrameHeader`, `FrameType`, and payload encoding, including schema bootstrap logic and CRC coverage; deliver unit tests plus fuzz targets for decode.
- **Backend integration**: implement the `WalStorage` facade on top of `fusio::Fs`, covering append, `sync_data`, rotation, and retention trimming while exposing structured errors for upstream.
- **Async writer**: implement `wal::writer` queue, sync policy evaluator, and `WalHandle` submission path; integrate metrics hooks and ensure bounded backpressure semantics.
- **DB integration**: wire `DB::enable_wal`/`disable_wal`, replay on open, and enqueue during ingest; add configuration plumbing so `DB` construction fails fast when the WAL is required but absent.
- **Recovery**: author `wal::replay::scan` and the fold that rebuilds mutable + immutable state; add validation tests for torn tails and mismatched schema digests.
- **Observability & QA**: surface metrics/logging, craft failure-injection tests (slow fsync, simulated backend errors), and document operational runbooks in `docs/`.

## Open Questions

- IPC vs. custom encoding for typed rows? (IPC batches unify both paths but may cost more on small appends.)
- Single WAL per DB vs. per-mode? (Prefer single; schema metadata disambiguates.)
- When to allow WAL truncation without SST/manifest? (Probably only manual for MVP.)

## Rollout Plan

1. Implement frame encode/decode + writer with `fsync` policy and rotation.
2. Integrate enqueue-on-ingest and ack based on policy.
3. Add recovery path on DB open.
4. Add tests + failure injection; add metrics and logs.
5. Document operational guidance and defaults.

## Future Work

- Checkpointing: periodic immutable snapshot + manifest, enabling WAL pruning.
- On-disk immutable segments (Arrow IPC/Parquet) and versioned manifest.
- Optional per-frame compression (zstd) and at-rest encryption.

## Appendix: Testing and Operational Guidance

### Testing Strategy

- **Unit tests**: Frame encode/decode, CRC verification, truncation behavior
- **Integration tests**: Insert and recover across crash boundaries; validate policy-driven seals
- **Failure injection**: Simulate partial headers/payloads; disable sync to measure expected RPO

### Operational Controls

| Category | Items |
|----------|-------|
| Counters | Frames/bytes written, fsync count and latency, queue depth, last durable sequence |
| Config knobs | Segment size (default 64 MiB), flush interval (default 10ms), sync policy, retention bytes, queue size |
| Logs | CRC failures, schema mismatch, rotation events, recovery truncation points |

### Configuration

WAL configuration covers:

| Setting | Purpose | Default |
|---------|---------|---------|
| Segment max bytes | Rotation threshold | 64 MiB |
| Flush interval | Coalesce timer | 10 ms |
| Sync policy | Durability vs performance trade-off | Interval-based (50ms) |
| Recovery mode | Behavior on corrupt frames | Point-in-time |
| Retention bytes | WAL space cap | Unlimited |
| Queue size | Backpressure threshold | 64k frames |

**Sync policies:**
- `Always`: fsync every flush (lowest RPO, highest latency)
- `IntervalBytes`: fsync after N bytes accumulated
- `IntervalTime`: fsync at most once per time interval
- `Disabled`: dev/test only; no durability guarantee

**Recovery modes** (aligned with RocksDB semantics):
- `PointInTime`: Stop at first truncated/unreadable frame
- `TolerateCorruptedTail`: Same as PointInTime for now
- `AbsoluteConsistency`: Abort on any I/O or codec error
- `SkipCorrupted`: Skip corrupt frames and continue (for disaster recovery)

### Runtime Semantics

- Single writer task per DB; ingest must pass through WAL before mutating in-memory state
- Backpressure: bounded queue blocks submitters when full
- Disabling WAL drains the writer and seals the active segment
- On I/O failure, DB transitions to error state and rejects further ingest

### Recovery Semantics

- Recovery scans WAL directory, validates frames by CRC, stops at first invalid frame
- Applies only committed transactions; uncommitted provisionals are discarded
- Initializes commit clock to `max(commit_ts) + 1` to maintain monotonicity
- Trims active segment to last valid frame before accepting new writes
