# RFC: Write-Ahead Log (WAL) for Tonbo

- Status: Draft
- Authors: Tonbo team
- Created: 2025-08-24
- Area: Durability, I/O, Recovery

## Summary

Introduce a write-ahead log (WAL) to make Tonbo durable against process crashes and power loss. The WAL persists ingest operations (currently Arrow `RecordBatch` payloads; typed row encoding is deferred but kept in the frame model) with a simple framed format, checksums, and controlled fsync policy. Recovery replays valid frames to rebuild the mutable state and reattach any immutables created prior to the crash. This RFC focuses on the WAL-only MVP (single-writer queue, no SSTs/manifest yet) and documents how it compares to the legacy implementation and how it aligns with the MVCC design in RFC 0003. Conditional publish (CAS) remains a hard requirement so multi-writer deployments can be enabled in follow-ups without redesigning the frame or storage model.

## Goals

- Crash/power-loss durability with bounded RPO depending on sync policy.
- Idempotent replay; preserve ingest order within a DB instance.
- Minimal coupling to the current in-memory design; future-proof for SST/manifest.

## Non-Goals (MVP)

- On-disk SST/segment formats and a persistent manifest (follow-up RFCs).
- WAL truncation/compaction beyond basic retention.
- Encryption/compression (can be added later).

## Legacy WAL (for context)

Path: `/Users/gwo/Idea/seren`

High-level model:

- Sequential log with framed records supporting fragmentation: `Full/First/Middle/Last` frames per logical record (refer to [the type design of RocksDB](https://github.com/facebook/rocksdb/wiki/Write-Ahead-Log-File-Format)).
- Records capture ingests and transactional groupings; recovery scans until the first invalid frame and replays valid records.
- Commit timestamps are logical and reassigned on recovery while preserving ingest order.

Selected mechanics:

- CRC-guarded frames; header conveys type and length.
- Transactions buffer local writes; WAL contains multi-frame records; commit makes them visible atomically.
- Recovery rebuilds in-memory structures and reassigns new logical timestamps; uncommitted records at the tail are discarded.

Correctness properties achieved via WAL + MVCC:

- Idempotent replay until a corruption/truncation boundary.
- Atomic visibility of multi-key commits at a single logical timestamp.
- Read-after-recovery preserves write order even if timestamps are reassigned.

## Legacy Risks and Gaps

- Timestamp identity: reassigning commit timestamps on recovery is acceptable for SI but must be documented and may complicate cross-process coordination later.
- Implicit commit grouping: fragment-oriented frames require higher-level grouping to infer commit boundaries; explicit commit markers are clearer for recovery.
- WAL pruning: robust truncation depends on an external snapshot/manifest; without it, logs may grow indefinitely.
- Dynamic schema coupling: schema mismatches lead to recovery failure; schema/versioning needs clear persistence and validation.
- Operational visibility: limited metrics around last durable sequence and recovery truncation points.

## Failure Model

- Crashes anywhere: during append, rotation, flush-to-immutable, or recovery.
- Partial writes: last frame may be incomplete; recovery stops at first invalid frame.
- Filesystem: durability gates enforced via `fsync`/`fdatasync`. No assumptions about cross-file ordering.

## WAL Frame Model

Common frame header (little-endian):
- magic: `u32` (e.g., 0x544F4E57 "TONW")
- version: `u16` (start with 1)
- frame_type: `u16`
- seq: `u64` (monotonic frame sequence)
- len: `u32` (payload length in bytes)
- crc32c: `u32` (header+payload or payload-only; pick one and document)

Frame types (payload formats):
- `TxnBegin { provisional_id: u64 }`: starts a transactional group. Optional in autocommit mode.
- `TxnAppend { provisional_id: u64, mode: Dyn, row_mode, payload }`: appends batches to the open txn.
  - Payload prefix: `provisional_id: u64`, `mode: u8`, `row_mode: u8`, `reserved: [u8; 6]`. The `row_mode` byte consumes the first slot of the reserved block that previously followed `mode`; remaining bytes stay zero until reused.
  - `mode=Dyn` retains the Arrow IPC message for dynamic `RecordBatch` payloads; the schema travels with every payload (IPC stream header + single batch message).
  - `mode=Typed` remains reserved so we can reintroduce compile-time typed dispatch without mutating the frame model; the encoding will mirror the legacy `(Key, Payload)` layout when reinstated.
  - Row modes (applies only to `mode=Dyn` for now):
    - `RowPayload (0)`: payload is the Arrow IPC batch containing all row columns plus an appended `_commit_ts` (`UInt64`) sidecar.
    - `KeyDelete (1)`: key-only deletes. Payload is an Arrow IPC batch encoded with the **delete schema**: the same primary-key columns defined by the extractor followed by a trailing `_commit_ts` (`UInt64`) column. `_tombstone` is never present. Writers must still encode per-row timestamps (constant columns are fine). Replayers reconstruct delete intents directly from the keys + commit timestamps without needing placeholder value columns.
  - Legacy segments implicitly carry `row_mode = RowPayload` because the reserved byte was zeroed; new decoders treat “missing byte” as `RowPayload` for forward/backward compatibility.
- `TxnCommit { provisional_id: u64, commit_ts: u64 }`: commits the txn and makes its mutations durable and visible atomically.
- `TxnAbort { provisional_id: u64 }` (optional): explicit abort; absence of commit on crash implies abort.
- `SealMarker` (optional): advisory boundary written when sealing occurs. Recovery can ignore this in MVP.

Autocommit fast path:

- An implementation MAY encode a single-operation autocommit as `TxnAppend` immediately followed by `TxnCommit` with a short-lived `provisional_id`, or define a convenience `AppendAutocommit { commit_ts, ... }` that is equivalent on recovery.

## File Layout and Rotation

- Directory: `wal/` under the DB root.
- File naming: `wal-<start_seq>.tonwal` (start sequence encoded in the name for ordering).
- File header: magic (`"TONWAL1"`), version, and optional metadata blocks:
  - dynamic-mode schema digest and optionally an embedded schema block (first file).
  - optional `LastCommitTs` control record; recovery can also derive it as `max(commit_ts)`.
- Rotation: on reaching `segment_max_bytes` or time-based; start a new file with header.
- Retention: MVP keeps recent N segments or caps total WAL bytes. Truncation becomes robust when a manifest/SST snapshot exists.

## Writer Pipeline and Sync Policy

- Single appender with bounded MPSC queue.
- Coalesce small frames; flush on size or timer (e.g., 5–20 ms).
- Sync policy:
  - `SyncAlways`: `fsync` every flush (lowest RPO, slowest).
  - `SyncInterval{ bytes, millis }`: sync on thresholds.
  - `SyncDisabled`: dev/test only.
- Error handling: on write/fsync error, surface to caller and potentially switch DB to read-only until resolved.
- Acknowledgement: in autocommit, ack after `TxnCommit` (or `AppendAutocommit`) is durably flushed per policy; in explicit transactions, ack `commit()` only after `TxnCommit` is durable.

## Recovery Algorithm

1. Discover WAL files by name; sort by `start_seq`. Validate headers.
2. Sequentially scan frames:
   - Verify `len` and `crc32c`; stop at the first invalid/incomplete frame.
   - Maintain an in-memory map `provisional_id -> buffered appends`.
   - `TxnBegin`: create an empty buffer for `provisional_id`.
   - `TxnAppend`: decode payload and append to the buffer for `provisional_id`.
   - `TxnCommit`: apply buffered appends to the mutable store using the provided `commit_ts`; clear the buffer.
   - `TxnAbort`: drop any buffered appends.
   - `SealMarker`: optional bookkeeping; can be ignored initially.
3. If recovery ends with open provisional txns (no commit), discard them.
4. Track last applied sequence and compute `last_commit_ts = max(commit_ts)` across commits; initialize the in-memory commit counter to `last_commit_ts + 1`.
5. On schema mismatch (dynamic), abort with a clear error.

### Transaction persistence behavior

- Live commits: the transaction façade buffers mutations (via `StagedMutations`), emits `TxnBegin/TxnAppend`, waits for the `TxnCommit` frame to become durable, then applies rows to the mutable memtable and publishes manifest edits. This guarantees every committed transaction has a WAL-backed proof before it touches in-memory state.
- Recovery: replay reconstructs the same transaction buffers from the WAL, and only once a durable `TxnCommit` is observed does the recovery flow hand the payload to the transaction logic to apply into the mutable memtable + manifest. Uncommitted provisionals never reach the memtable.
- Result: the effective pipeline becomes `WAL -> transaction -> mutable memtable` (rather than the legacy direct WAL→memtable apply), so durability and snapshot semantics are identical whether the system is running normally or recovering.

## Integration Points

- DB init (dynamic): first WAL segment persists schema and extractor metadata (e.g., key column index + type). Validate on open.
- DB init (typed): persist derived Arrow schema or a type hash for diagnostics.
- Insert path: enqueue WAL frame before acknowledging ingest; acknowledgment timing depends on sync policy.
- Sealing: optionally log `SealMarker` when sealing occurs. Flushing to immutables remains in-memory for MVP; WAL is the durability source.
- MVCC alignment: the `commit_ts` written in `TxnCommit` is the MVCC `begin_ts` for versions; see RFC 0003. Autocommit uses one `commit_ts` per ingest; explicit transactions assign one `commit_ts` to all buffered mutations.

## Legacy and External Reference

| Aspect | Legacy Tonbo WAL | RFC Direction | External Inspiration |
| --- | --- | --- | --- |
| Framing | `Full/First/Middle/Last` fragments encode multi-record batches on a sequential log | `TxnBegin/TxnAppend/TxnCommit` frames keep fragmenting for large payloads but make commit intent explicit | RocksDB fragments WriteBatches per 32 KiB block and reassembles them for atomic replay |
| Commit timestamps | Logical timestamps reassigned during recovery | `commit_ts` persisted per transaction; recovery seeds `commit_clock` from the max observed value | RocksDB TransactionDB records commit metadata so recovery never invents timestamps |
| Object storage flow | Legacy flushed via local fusio logger then copied into remote storage | `WalStorage` relies on fusio's uniform append/commit semantics regardless of backend | RocksDB recommends staging WAL locally before uploading, but fusio now abstracts that detail |
| Multi-family support | Single family; typed modes planned | Frame header will carry logical family/mode identifiers so one WAL serves multiple memtables atomically | RocksDB tags each WriteBatch entry with a column-family ID inside a shared WAL |
| Retention control | Manual or implicit; manifest unaware | Manifest / `wal/state.json` will record highest durable sequence per family before reclaiming segments | RocksDB MANIFEST maintains `min_flush_seq` to gate WAL deletion |
| Telemetry & knobs | Minimal observability | `WalConfig` exposes queue size, sync policy, rotation; metrics track queue depth, bytes, fsync latency | RocksDB surfaces WAL size, write stall, and sync counters; mirroring this keeps ops expectations aligned |

## MVCC Alignment and Changes

- Timestamp type: use `u64` for `commit_ts` to match RFC 0003.
- Visibility: recovery applies only committed txns; incomplete txns are ignored, matching snapshot isolation.
- Autocommit vs transactions: MVP may start with autocommit-only encoding; the frame model is forward-compatible with explicit transactions without changing recovery rules.

## Implementation Outline

We will implement the WAL on top of the `fusio` I/O layer so Tonbo can target any supported storage backend without bespoke branching while staying fully asynchronous.

### Async writer and executor

- WAL APIs (`DB::enable_wal`, `_wal.append_frame`, recovery) become `async`. The WAL service owns an
`Executor + Timer` handle (`fusio::executor`) so it can spawn background tasks without tying Tonbo to a specific runtime. A bounded channel feeds frames from ingest into the writer task; back pressure surfaces when the writer falls behind.
- The writer task tracks ingest order and materializes frames into `Vec<u8>` payloads (dynamic today, typed once reinstated) before handing them to the storage backend.

### Backend abstraction

- Wrap `Arc<dyn fusio::DynFs>` inside a lightweight `WalStorage` facade. Rely directly on fusio's `DynFile`/`DynWrite` traits for append, flush, and close semantics instead of layering a custom backend trait.
- Provide helpers for segment naming, rotation, and retention that operate purely through the fusio API surface; backend-specific behaviors (append vs. multipart upload, durability guarantees, etc.) remain encapsulated by fusio itself.
- Recovery logic reuses the same fusio handles to enumerate segments, stream frames with `Read::read_exact_at`, stop at the first invalid CRC, and rebuild mutable state.

### Sync policy mapping

- `WalSyncPolicy` is enforced in the writer by counting bytes and/or time since the last durability call:
  - `Always` ⇒ call `sync_all` after each committed frame (or after the enclosing transaction).
  - `IntervalBytes(n)` ⇒ accumulate byte count and call `sync_data` once the threshold is exceeded.
  - `IntervalTime(dt)` ⇒ use the executor’s timer to trigger `sync_data` at most once per interval.
- `Disabled` ⇒ rely on backend buffering; `seal` still calls `sync_all`/`commit`.
- If a backend reports limited durability capabilities through fusio, the writer translates the configured policy to the closest supported variant and emits telemetry so operators understand the trade-off.

### Backpressure and metrics

- The bounded channel between ingest and writer naturally applies backpressure. Metrics track queue depth, bytes written, sync counts, and the gap between enqueued and durable sequence numbers.
- The writer task emits structured logs when sync calls exceed a latency budget or when durability is downgraded by the backend.

### Testing strategy

- Unit tests for frame encode/decode and CRC.
- Integration tests across multiple fusio backends:
  - dynamic `RecordBatch` append/replay
  - truncation resilience (simulate torn tail)
  - sync policy adherence (inject slow sync + check counters)
- Future: typed row append/replay once compile-time dispatch returns.

## Engineering Task Breakdown

- **Frame & codec**: finalize the `FrameHeader`, `FrameType`, and payload encoding, including schema bootstrap logic and CRC coverage; deliver unit tests plus fuzz targets for decode.
- **Backend integration**: implement the `WalStorage` facade on top of `fusio::Fs`, covering append, `sync_data`, rotation, and retention trimming while exposing structured errors for upstream.
- **Async writer**: implement `wal::writer` queue, sync policy evaluator, and `WalHandle` submission path; integrate metrics hooks and ensure bounded backpressure semantics.
- **DB integration**: wire `DB::enable_wal`/`disable_wal`, replay on open, and enqueue during ingest; add configuration plumbing so `DB` construction fails fast when the WAL is required but absent.
- **Recovery**: author `wal::replay::scan` and the fold that rebuilds mutable + immutable state; add validation tests for torn tails and mismatched schema digests.
- **Observability & QA**: surface metrics/logging, craft failure-injection tests (slow fsync, simulated backend errors), and document operational runbooks in `docs/`.

## Open Questions

- IPC vs. custom encoding for typed rows? (IPC batches unify both paths but may cost more on small appends.)
- Single WAL per DB vs. per-mode? (Prefer single; schema metadata disambiguates.)
- When to allow WAL truncation without SST/manifest? (Probably only manual for MVP.)

## Rollout Plan

1. Implement frame encode/decode + writer with `fsync` policy and rotation.
2. Integrate enqueue-on-ingest and ack based on policy.
3. Add recovery path on DB open.
4. Add tests + failure injection; add metrics and logs.
5. Document operational guidance and defaults.

## Future Work

- Checkpointing: periodic immutable snapshot + manifest, enabling WAL pruning.
- On-disk immutable segments (Arrow IPC/Parquet) and versioned manifest.
- Optional per-frame compression (zstd) and at-rest encryption.

## Appendices

### Appendix A: Testing and Failure Injection

- Unit: headers, frame encode/decode, CRC verification, truncation behavior.
- Integration:
  - Typed: insert rows, hard-stop before seal; reopen and recover; keys match.
  - Dynamic: insert batch, truncate last frame; replay all valid frames only.
  - Policies: ensure policy-driven seals do not break recovery; immutables may be missing post-crash but mutable is correct.
- Failure injection: simulate partial headers/payloads; disable sync to measure expected RPO.

### Appendix B: Operational Controls and Metrics

- Counters: frames/bytes written, fsync count and latency, queue depth, last durable seq.
- Config knobs: segment size, flush interval, sync policy, retention bytes, queue size.
- Logs: CRC failures, schema mismatch, rotation start/end, recovery truncation points.

### Appendix C: Configuration API Sketch (Rust)

```rust
pub enum WalSyncPolicy {
  Always,           // fsync every flush
  IntervalBytes(usize),    // fsync after N bytes
  IntervalTime(std::time::Duration),
  Disabled,          // dev/test only
}

pub struct WalConfig {
  pub dir: std::path::PathBuf,  // e.g., db_root.join("wal")
  pub segment_max_bytes: usize, // e.g., 64 MiB
  pub flush_interval: std::time::Duration, // e.g., 10 ms
  pub sync: WalSyncPolicy,
  /// How recovery reacts to partially written or corrupt frames.
  pub recovery: WalRecoveryMode,
  pub retention_bytes: Option<usize>,   // None = keep all
  pub queue_size: usize,          // e.g., 64k frames
}

impl Default for WalConfig {
  fn default() -> Self {
    Self {
      dir: std::path::PathBuf::from("wal"),
      segment_max_bytes: 64 * 1024 * 1024,
      flush_interval: std::time::Duration::from_millis(10),
      sync: WalSyncPolicy::IntervalTime(std::time::Duration::from_millis(50)),
      recovery: WalRecoveryMode::PointInTime,
      retention_bytes: None,
      queue_size: 65_536,
    }
  }
}

/// Recovery behavior mirrors RocksDB so operators can adopt familiar policies.
pub enum WalRecoveryMode {
  /// Stop at the first truncated or unreadable frame and surface data up to that point.
  PointInTime,
  /// Ignore corruption at the tail of the active log (treated the same as `PointInTime` for now).
  TolerateCorruptedTail,
  /// Abort replay on the first I/O or codec error (TODO).
  AbsoluteConsistency,
  /// Skip corrupt frames and continue replaying later events (TODO).
  SkipCorrupted,
}

// Sketchy builder-style usage (no code yet):
// let exec = std::sync::Arc::new(fusio::executor::BlockingExecutor::default());
// let config = DynModeConfig::from_key_name(schema, "id")?;
// let mut db = DB::new(config, exec)?;
// db.enable_wal(WalConfig { dir: root.join("wal"), ..Default::default() });
//
// To opt into a different recovery mode:
// let mut cfg = WalConfig::default();
// cfg.dir = root.join("wal");
// cfg.recovery = WalRecoveryMode::TolerateCorruptedTail;
// db.enable_wal(cfg);
// db.ingest(batch).await?; // ack depends on sync policy
```

### Appendix D: API Surface, Modules, and Runtime Semantics

#### Module layout
- `src/wal/mod.rs`: public entry points (`WalConfig`, `WalHandle`, error types) and re-exports.
- `src/wal/frame.rs`: frame header/payload structs, versioning, CRC helpers.
- `src/wal/mod.rs`: public entry points (`WalConfig`, `WalHandle`, `WalTicket`, `WalError`) and the future writer loop (currently `Unimplemented`).
- `src/wal/backend.rs`: `WalBackend` trait plus the POSIX filesystem implementation.
- `src/wal/replay.rs`: recovery scanner that yields decoded `WalEvent`s back to `DB`.
- `src/wal/metrics.rs`: telemetry structs/hooks; compiled out when metrics feature disabled.

#### Integration status and ownership
- Public API (`WalConfig`, `WalSyncPolicy`, `WalHandle::submit/rotate`, `WalTicket::durable`, `WalPayload`, `WalAck`) is now frozen; request CTO review before altering signatures or semantics.
- Current scaffolding wires the handle into `DB::enable_wal`/`disable_wal` and exposes storage/replay entry points. The writer queue, storage operations, sync policy enforcement, and recovery scanning are intentionally left as `WalError::Unimplemented` for the WAL feature team to implement.
- `WalStorage` must eventually cover directory bootstrap, segment lifecycle, and retention; `Replayer::scan` will drive recovery. Add concrete behavior plus tests in follow-on patches while preserving the documented API surface.
- `WalMetrics` exists as a placeholder struct—hook counters into the writer/storage once they land and extend the struct as telemetry gaps appear.

#### Public API (Rust)

```rust
pub use wal::{WalConfig, WalSyncPolicy, WalHandle, WalTicket, WalError};

impl<M: Mode, E: fusio::executor::Executor + fusio::executor::Timer> DB<M, E> {
  pub async fn enable_wal(&mut self, cfg: WalConfig) -> Result<WalHandle<E>, WalError>;

  pub async fn disable_wal(&mut self) -> Result<(), WalError>;

  pub fn wal(&self) -> Option<&WalHandle<E>>;
}

pub(crate) struct WalPayload {
  batch: RecordBatch,
  tombstones: Vec<bool>,
  commit_ts: u64,
}

impl<E> WalHandle<E> {
  pub async fn submit(&self, payload: WalPayload) -> Result<WalTicket<E>, WalError>;
  pub async fn rotate(&self) -> Result<(), WalError>; // manual rotation hook
}

pub struct WalTicket<E> {
  pub seq: u64, // frame sequence assigned on enqueue
}

impl<E> WalTicket<E> {
  pub async fn durable(&self) -> Result<WalAck, WalError>; // resolves when fsync policy satisfied
}

pub struct WalAck {
  pub first_seq: u64, // first frame written by this command
  pub last_seq: u64,  // final frame written by this command
  pub bytes_flushed: usize,
  pub elapsed: Duration,
}
```

Key semantics:
- `enable_wal` installs the writer task and replays existing segments before returning the handle.
- `WalHandle::submit` awaits enqueueing (and any backpressure) before returning the ticket; the caller can then `await` `WalTicket::durable()` to honor the configured durability policy.
- `WalTicket::durable().await` resolves with an `Ack` whose `first_seq..=last_seq` span covers every frame the command wrote. Ingest paths must feed those spans into `WalRangeAccumulator` / mutable+immutable tracking so GC never trims still-live frames.
- The `bytes_flushed` and `elapsed` fields continue to describe durability latency and throughput; surface them to users when needed.

#### Runtime semantics
- Only one writer task exists per `DB`. Ingest operations must pass through the WAL before mutating in-memory state; a write is considered visible once the ticket resolves.
- Backpressure: when the bounded queue is full, `submit` blocks (async) until capacity exists, so user-facing ingest futures naturally wait.
- Disabling the WAL awaits the writer to drain, seals the active segment, and prevents new submit calls. Re-enabling replays again.
- Error propagation: `WalError::Storage` surfaces permanent I/O failures; the DB transitions into `ErrState` and rejects further ingest until reset.

### Delete batch schema

- Column order mirrors the extractor’s primary-key projection followed by `_commit_ts`.
- There are no value columns and no nullable key components (same constraints as upsert batches).
- Writers can reuse the same Arrow schema object for all delete batches; a schema digest in the WAL header should list both the upsert and delete schemas when the feature is enabled.

#### Recovery semantics
- `replay::scan(cfg.dir)` produces `(WalEvent, seq)` tuples ordered by `seq`, stopping at the first invalid frame. `RowPayload` frames decode to `WalEvent::DynAppend` (batch + `_commit_ts`), while `KeyDelete` frames decode to `WalEvent::DynDelete` that carries the delete schema batch.
- Recovery behavior is controlled by `WalRecoveryMode`. The default `PointInTime` (and `TolerateCorruptedTail`) mode returns all frames written before the first truncated or corrupt tail record. `AbsoluteConsistency` and `SkipCorrupted` are placeholders that will fail fast until their semantics are implemented.
- Startup sanitation trims the active segment back to the last fully decoded frame so new writes never append past corrupt bytes.
- The `DB` applies each event by reusing the existing dynamic ingest path; this guarantees replay observes the same validation logic as live ingest.
- Recovery records the highest `commit_ts` and seed sequence numbers so subsequent appends remain monotonic.
- Optional seal markers are advisory; missing markers do not block recovery.

#### Observability hooks
- `WalMetrics` exposes queue depth, pending vs durable sequence, sync latency p95/p99, and number of degraded-policy downgrades reported by backends.
- Structured logs (`wal::event::LogRecord`) capture rotations, sync thresholds, and recovery truncation decisions and are emitted through the shared tracing facade.
- Feature flag `metrics` controls whether metrics glue compiles; the API remains present but returns no-op handles without the feature.
